{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.config import TRANSFORMED_DATA_DIR\n",
    "from src.data_utils import split_time_series_data\n",
    "from src.experiment_utils import set_mlflow_tracking, log_model_to_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55900, 674)\n",
      "(55900,)\n",
      "(31720, 674)\n",
      "(31720,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(TRANSFORMED_DATA_DIR / \"tabular_data.parquet\")\n",
    "df.head(5)\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_time_series_data(\n",
    "    df,\n",
    "    cutoff_date=datetime(2023, 9, 1, 0, 0, 0),\n",
    "    target_column=\"target\"\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "past_ride_columns = [c for c in X_train.columns if c.startswith(\"rides_\")]\n",
    "X_train_only_numeric = X_train[past_ride_columns]\n",
    "X_test_only_numeric = X_test[past_ride_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rides_last_4_weeks(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    last_4_weeks_columns = [\n",
    "            f\"rides_t-{7*24}\",  # 1 week ago\n",
    "            f\"rides_t-{14*24}\", # 2 weeks ago\n",
    "            f\"rides_t-{21*24}\", # 3 weeks ago\n",
    "            f\"rides_t-{28*24}\"  # 4 weeks ago\n",
    "        ]\n",
    "\n",
    "        # Ensure the required columns exist in the test DataFrame\n",
    "    for col in last_4_weeks_columns:\n",
    "        if col not in X.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Calculate the average of the last 4 weeks\n",
    "    X[\"average_rides_last_4_weeks\"] = X[last_4_weeks_columns].mean(axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "add_feature_average_rides_last_4_weeks = FunctionTransformer(\n",
    "    average_rides_last_4_weeks, validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom scikit-learn based class that extracts the temporal features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        X_[\"hour\"] = X_[\"pickup_hour\"].dt.hour\n",
    "        X_[\"day_of_week\"] = X_[\"pickup_hour\"].dt.dayofweek\n",
    "\n",
    "        return X_.drop(columns=[\"pickup_hour\", \"pickup_location_id\"])\n",
    "\n",
    "add_temporal_features = TemporalFeatureEngineer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    add_feature_average_rides_last_4_weeks,\n",
    "    add_temporal_features,\n",
    "    lgb.LGBMRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running LGM with Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\Downloads\\CDS500_Applied_ML_DS\\Projects\\CDA500P1\\CDA500P1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 156723\n",
      "[LightGBM] [Info] Number of data points in the train set: 37266, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 13.482799\n",
      "[CV] END ........................lgbmregressor__num_leaves=2; total time=   2.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153420\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 8.169855\n",
      "[CV] END ........................lgbmregressor__num_leaves=2; total time=   2.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 155219\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 11.678268\n",
      "[CV] END ........................lgbmregressor__num_leaves=2; total time=   2.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 156723\n",
      "[LightGBM] [Info] Number of data points in the train set: 37266, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 13.482799\n",
      "[CV] END .......................lgbmregressor__num_leaves=50; total time=   4.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153420\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 8.169855\n",
      "[CV] END .......................lgbmregressor__num_leaves=50; total time=   4.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 155219\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 11.678268\n",
      "[CV] END .......................lgbmregressor__num_leaves=50; total time=   5.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 156723\n",
      "[LightGBM] [Info] Number of data points in the train set: 37266, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 13.482799\n",
      "[CV] END .......................lgbmregressor__num_leaves=70; total time=   5.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153420\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 8.169855\n",
      "[CV] END .......................lgbmregressor__num_leaves=70; total time=   6.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 155219\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 11.678268\n",
      "[CV] END .......................lgbmregressor__num_leaves=70; total time=   6.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101752 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 156723\n",
      "[LightGBM] [Info] Number of data points in the train set: 37266, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 13.482799\n",
      "[CV] END ......................lgbmregressor__num_leaves=256; total time=  12.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 153420\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 8.169855\n",
      "[CV] END ......................lgbmregressor__num_leaves=256; total time=  15.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.131029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 155219\n",
      "[LightGBM] [Info] Number of data points in the train set: 37267, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 11.678268\n",
      "[CV] END ......................lgbmregressor__num_leaves=256; total time=  17.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 159913\n",
      "[LightGBM] [Info] Number of data points in the train set: 55900, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 11.110286\n",
      "Best Parameters: {'lgbmregressor__num_leaves': 50}\n",
      "Best Score (Negative MAE): -2.5693649275573294\n",
      "Test Set MAE: 3.223177653063996\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    \"lgbmregressor__num_leaves\": [2, 50, 70, 256],\n",
    "    # \"lgbmregressor__max_depth\": [-1, 10, 20, 30],\n",
    "    # \"lgbmregressor__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    # \"lgbmregressor__n_estimators\": [100, 200, 500, 1000],\n",
    "    # \"lgbmregressor__min_child_samples\": [10, 20, 30, 50],\n",
    "    # \"lgbmregressor__subsample\": [0.6, 0.8, 1.0],\n",
    "    # \"lgbmregressor__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    # \"lgbmregressor__reg_alpha\": [0, 0.1, 0.5, 1.0],\n",
    "    # \"lgbmregressor__reg_lambda\": [0, 0.1, 0.5, 1.0],\n",
    "    # \"lgbmregressor__feature_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "    # \"lgbmregressor__bagging_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "    # \"lgbmregressor__bagging_freq\": [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=5,  # Number of parameter settings sampled\n",
    "    scoring=\"neg_mean_absolute_error\",  # Use MAE as the scoring metric\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test Set MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging model and results in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.experiment_utils:MLflow tracking URI and credentials set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.experiment_utils:Experiment set to: LGBMRegressorHyper\n",
      "INFO:src.experiment_utils:Logged mean_absolute_error: 3.223177653063996\n",
      "c:\\Users\\singh\\Downloads\\CDS500_Applied_ML_DS\\Projects\\CDA500P1\\CDA500P1\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "INFO:src.experiment_utils:Model signature inferred.\n",
      "c:\\Users\\singh\\Downloads\\CDS500_Applied_ML_DS\\Projects\\CDA500P1\\CDA500P1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 100.00it/s] \n",
      "2025/03/04 17:28:04 INFO mlflow.models.model: Found the following environment variables used during model inference: [HOPSWORKS_API_KEY]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n",
      "Registered model 'Pipeline' already exists. Creating a new version of this model...\n",
      "2025/03/04 17:30:59 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Pipeline, version 4\n",
      "Created version '4' of model 'Pipeline'.\n",
      "INFO:src.experiment_utils:Model logged with name: Pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run beautiful-duck-481 at: https://dagshub.com/singhvarunnn789/CDA500P1.mlflow/#/experiments/6/runs/4f4d01cc004f4035a3ea3c88103bf623\n",
      "🧪 View experiment at: https://dagshub.com/singhvarunnn789/CDA500P1.mlflow/#/experiments/6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x1da928d3e30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = best_model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"{test_mae:.4f}\")\n",
    "mlflow = set_mlflow_tracking()\n",
    "log_model_to_mlflow(best_model, X_test, \"LGBMRegressorHyper\", \"mean_absolute_error\", score=test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Hyperparameter**       | **Description**                                                                                                                                                                                                 | **Impact**                                                                                                                                                                                                                     |\n",
    "|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `num_leaves`             | Maximum number of leaves in one tree. Larger values increase model complexity and accuracy but may lead to overfitting.                                                                                         | High: Directly controls the complexity of the model. Larger values can improve accuracy but increase the risk of overfitting.                                                                                                  |\n",
    "| `max_depth`              | Maximum depth of a tree. Limits the depth of the tree to prevent overfitting. Use `-1` for no limit.                                                                                                           | High: Limits the model's ability to capture complex patterns. Shallower trees reduce overfitting but may underfit.                                                                                                             |\n",
    "| `learning_rate`          | Step size for updating weights. Smaller values make training slower but improve generalization.                                                                                                                | High: Affects convergence speed and generalization. Lower values often require more iterations (`n_estimators`).                                                                                                               |\n",
    "| `n_estimators`           | Number of boosting iterations (trees). Higher values improve accuracy but increase training time.                                                                                                              | High: Controls the number of trees in the ensemble. Too many trees can lead to overfitting if `learning_rate` is not adjusted.                                                                                                |\n",
    "| `feature_fraction`       | Fraction of features (columns) to randomly select for each tree. Helps prevent overfitting.                                                                                                                    | Medium-High: Reduces overfitting and speeds up training. Lower values may lead to underfitting.                                                                                                                               |\n",
    "| `bagging_fraction`       | Fraction of data (rows) to randomly select for each iteration. Works with `bagging_freq`.                                                                                                                       | Medium-High: Reduces overfitting and improves generalization. Lower values may lead to underfitting.                                                                                                                           |\n",
    "| `bagging_freq`           | Frequency of bagging. For example, `5` means bagging is performed every 5 iterations.                                                                                                                          | Medium: Works with `bagging_fraction` to control how often bagging is applied.                                                                                                                                                |\n",
    "| `min_child_samples`      | Minimum number of data points required in a leaf. Larger values prevent overfitting by limiting leaf size.                                                                                                     | Medium: Controls overfitting by limiting the size of leaf nodes. Higher values may lead to underfitting.                                                                                                                       |\n",
    "| `colsample_bytree`       | Alias for `feature_fraction`. Fraction of features to randomly select for each tree.                                                                                                                            | Medium: Similar to `feature_fraction`, reduces overfitting and speeds up training.                                                                                                                                            |\n",
    "| `subsample`              | Alias for `bagging_fraction`. Fraction of data to randomly select for each iteration.                                                                                                                           | Medium: Similar to `bagging_fraction`, reduces overfitting and improves generalization.                                                                                                                                       |\n",
    "| `reg_alpha`              | L1 regularization term on weights. Adds a penalty for large coefficients to reduce overfitting.                                                                                                                | Medium: Helps reduce overfitting, especially in high-dimensional data.                                                                                                                                                        |\n",
    "| `reg_lambda`             | L2 regularization term on weights. Adds a penalty for large coefficients to reduce overfitting.                                                                                                                | Medium: Similar to `reg_alpha`, but penalizes squared coefficients.                                                                                                                                                           |\n",
    "| `max_bin`                | Maximum number of bins for discretizing continuous features. Higher values improve accuracy but increase training time.                                                                                         | Medium: Affects how continuous features are bucketed. Higher values improve precision but increase computational cost.                                                                                                         |\n",
    "| `min_split_gain`         | Minimum gain required to split a node. Higher values prevent splitting nodes with low information gain.                                                                                                         | Medium-Low: Helps control overfitting by limiting unnecessary splits.                                                                                                                                                         |\n",
    "| `boosting_type`          | Type of boosting algorithm. Options: `gbdt` (default), `dart`, `goss`.                                                                                                                                          | Medium-Low: Affects the boosting strategy. `dart` and `goss` are alternatives to `gbdt` for specific use cases.                                                                                                               |\n",
    "| `objective`              | Objective function to optimize. Common options: `regression`, `regression_l1`, `huber`, `fair`.                                                                                                                | Medium-Low: Determines the loss function. Impacts how the model optimizes predictions.                                                                                                                                         |\n",
    "| `verbosity`              | Controls the level of logging. Higher values provide more detailed logs.                                                                                                                                       | Low: Does not affect model performance but helps with debugging.                                                                                                                                                              |\n",
    "| `random_state`           | Seed for reproducibility. Ensures consistent results across runs.                                                                                                                                               | Low: Does not affect model performance but ensures reproducibility.                                                                                                                                                           |\n",
    "| `early_stopping_round`   | Stops training if validation score does not improve for a specified number of rounds.                                                                                                                           | Low: Helps save time during training but does not directly affect model performance.                                                                                                                                           |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDA500P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
