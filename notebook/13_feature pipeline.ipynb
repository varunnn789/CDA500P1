{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.data_utils import load_and_process_taxi_data\n",
    "from src.data_utils import transform_raw_data_into_ts_data\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting current date, and setting timelines for data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 10:38:26.955242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\AppData\\Local\\Temp\\ipykernel_32484\\3971000852.py:1: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_date = pd.to_datetime(datetime.utcnow())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-04 10:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = pd.to_datetime(datetime.utcnow())\n",
    "print(f\"{current_date}\")\n",
    "type(current_date)\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).floor(\"h\")\n",
    "current_date.to_datetime64()\n",
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days=29)\n",
    "fetch_data_to\n",
    "fetch_data_from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition for fetching data in a batch for specified timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for 2024-02.\n",
      "Loading data for 2024-02...\n",
      "Total records: 3,007,526\n",
      "Valid records: 2,954,709\n",
      "Records dropped: 52,817 (1.76%)\n",
      "Successfully processed data for 2024-02.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "File already exists for 2024-03.\n",
      "Loading data for 2024-03...\n",
      "Total records: 3,582,628\n",
      "Valid records: 3,518,066\n",
      "Records dropped: 64,562 (1.80%)\n",
      "Successfully processed data for 2024-03.\n",
      "Combining all monthly data...\n",
      "Data loading and processing complete!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175392 entries, 0 to 175391\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   pickup_hour         175392 non-null  datetime64[ns]\n",
      " 1   pickup_location_id  175392 non-null  int16         \n",
      " 2   rides               175392 non-null  int16         \n",
      "dtypes: datetime64[ns](1), int16(2)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "def fetch_batch_raw_data(from_date: Union[datetime, str], to_date: Union[datetime, str]) -> pd.DataFrame:\n",
    "    if isinstance(from_date, str):\n",
    "        from_date = datetime.fromisoformat(from_date)\n",
    "    if isinstance(to_date, str):\n",
    "        to_date = datetime.fromisoformat(to_date)\n",
    "\n",
    "    if from_date >= to_date:\n",
    "        raise ValueError(\"'from_date' must be earlier than 'to_date'.\")\n",
    "\n",
    "    # Shift dates back by 52 weeks and remove timezone info\n",
    "    historical_from_date = (from_date - timedelta(weeks=52)).replace(tzinfo=None)\n",
    "    historical_to_date = (to_date - timedelta(weeks=52)).replace(tzinfo=None)\n",
    "\n",
    "    rides_from = load_and_process_taxi_data(year=historical_from_date.year, months=[historical_from_date.month])\n",
    "    rides_from['pickup_datetime'] = rides_from['pickup_datetime'].dt.tz_localize(None)\n",
    "    rides_from = rides_from[rides_from.pickup_datetime >= historical_from_date]\n",
    "\n",
    "    if historical_to_date.month != historical_from_date.month:\n",
    "        rides_to = load_and_process_taxi_data(year=historical_to_date.year, months=[historical_to_date.month])\n",
    "        rides_to['pickup_datetime'] = rides_to['pickup_datetime'].dt.tz_localize(None)\n",
    "        rides_to = rides_to[rides_to.pickup_datetime < historical_to_date]\n",
    "        rides = pd.concat([rides_from, rides_to], ignore_index=True)\n",
    "    else:\n",
    "        rides = rides_from\n",
    "\n",
    "    rides['pickup_datetime'] += timedelta(weeks=52)\n",
    "    rides.sort_values(by=['pickup_location_id', 'pickup_datetime'], inplace=True)\n",
    "    \n",
    "    return rides\n",
    "\n",
    "\n",
    "rides = fetch_batch_raw_data(fetch_data_from, fetch_data_to)\n",
    "rides.head(5)\n",
    "ts_data = transform_raw_data_into_ts_data(rides)\n",
    "ts_data.head(5)\n",
    "ts_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopsworks Login and loading feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\Downloads\\CDS500_Applied_ML_DS\\Projects\\CDA500P1\\CDA500P1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 05:38:36,191 INFO: Initializing external client\n",
      "2025-03-05 05:38:36,195 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-05 05:38:37,411 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214689\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# connect to the project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# connect to the feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# connect to the feature group\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    "    description=\"Time series data at hourly freaquency\",\n",
    "    primary_key=[\"pickup_location_id\", \"pickup hour\"],\n",
    "    event_time=\"pickup_hour\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 175392/175392 | Elapsed Time: 00:11 | Remaining Time: 00:00\n",
      "UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1214689/jobs/named/time_series_hourly_feature_group_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/CDA500P1/Resources/jobs/time_series_hourly_feature_group_1_offline_fg_materialization/config_1741118123424) to trigger the materialization job again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('time_series_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.insert(ts_data, write_options={\"wait_for_job\": False})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDA500P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
